"""
Optimized Metrics Computation for CS2 Demo Analysis

This module provides vectorized, high-performance implementations of:
- Time to Damage (TTD): Vectorized using pandas groupby
- Crosshair Placement (CP): Vectorized using numpy broadcasting

Optimizations:
1. Vectorized operations instead of per-row loops
2. Numpy broadcasting for angular calculations
3. Configurable/lazy metric computation
4. Caching support for repeated analysis

Performance improvements:
- TTD: ~10-50x faster on large demos
- CP: ~5-20x faster with batch angular calculations
"""

from __future__ import annotations

import hashlib
import json
import logging
from dataclasses import dataclass, field
from enum import Flag, auto
from pathlib import Path
from typing import TYPE_CHECKING, Any

import numpy as np
import pandas as pd

if TYPE_CHECKING:
    pass

from opensight.core.constants import CS2_TICK_RATE

# Weapons that are NOT valid for crosshair placement measurement
# (knives, grenades, taser, c4)
_INVALID_CP_WEAPONS = frozenset(
    {
        "knife",
        "knife_t",
        "knifegg",
        "bayonet",
        "knife_m9_bayonet",
        "knife_karambit",
        "knife_butterfly",
        "knife_flip",
        "knife_gut",
        "knife_tactical",
        "knife_falchion",
        "knife_push",
        "knife_survival_bowie",
        "knife_ursus",
        "knife_gypsy_jackknife",
        "knife_stiletto",
        "knife_widowmaker",
        "knife_css",
        "knife_cord",
        "knife_canis",
        "knife_outdoor",
        "knife_skeleton",
        "hegrenade",
        "flashbang",
        "smokegrenade",
        "molotov",
        "incgrenade",
        "decoy",
        "taser",
        "c4",
    }
)


def _is_valid_cp_weapon(weapon: str | None) -> bool:
    """Check if weapon is valid for crosshair placement calculation.

    Args:
        weapon: Weapon name (e.g., "ak47", "knife", "hegrenade")

    Returns:
        True if weapon is valid for CP (hitscan weapons), False otherwise
    """
    if not weapon:
        return False
    weapon_lower = weapon.lower()
    # Check for knife prefix (catches all knife variants)
    if weapon_lower.startswith("knife"):
        return False
    return weapon_lower not in _INVALID_CP_WEAPONS


logger = logging.getLogger(__name__)

# Constants
MS_PER_TICK = 1000 / CS2_TICK_RATE  # ~15.625ms at 64 tick
TTD_MIN_MS = 0
TTD_MAX_MS = 1500
CP_MAX_DISTANCE = 2000  # Max distance for CP calculation (units)
CP_MIN_DISTANCE = 50  # Min distance to avoid division issues
CP_MAX_SHOTS_FOR_VALID = 3  # Max shots to count as "clean" kill (filter sprays)


class MetricType(Flag):
    """Flags for which metrics to compute."""

    NONE = 0
    TTD = auto()  # Time to Damage
    CP = auto()  # Crosshair Placement
    KAST = auto()  # Kill/Assist/Survived/Traded
    TRADES = auto()  # Trade kills
    OPENING_DUELS = auto()  # Opening duels
    MULTI_KILLS = auto()  # Multi-kill rounds
    CLUTCHES = auto()  # Clutch situations
    UTILITY = auto()  # Utility usage
    ACCURACY = auto()  # Shot accuracy
    ECONOMY = auto()  # Economy stats
    SIDES = auto()  # CT/T side breakdown
    MISTAKES = auto()  # Team damage/kills

    # Preset combinations
    BASIC = KAST | TRADES | OPENING_DUELS | MULTI_KILLS
    ADVANCED = BASIC | TTD | CP | CLUTCHES | UTILITY
    FULL = ADVANCED | ACCURACY | ECONOMY | SIDES | MISTAKES


@dataclass
class TTDMetrics:
    """Vectorized TTD computation results."""

    # Per-kill TTD values (attacker_steamid -> list of TTD_ms)
    player_ttd_values: dict[int, list[float]] = field(default_factory=dict)
    # Per-kill prefire counts
    player_prefire_counts: dict[int, int] = field(default_factory=dict)
    # Total engagements analyzed
    total_engagements: int = 0

    def get_median(self, steamid: int) -> float | None:
        """Get median TTD for a player."""
        values = self.player_ttd_values.get(steamid, [])
        return float(np.median(values)) if values else None

    def get_mean(self, steamid: int) -> float | None:
        """Get mean TTD for a player."""
        values = self.player_ttd_values.get(steamid, [])
        return float(np.mean(values)) if values else None


@dataclass
class CPMetrics:
    """Vectorized CP computation results."""

    # Per-kill CP values (attacker_steamid -> list of angular_error_deg)
    player_cp_values: dict[int, list[float]] = field(default_factory=dict)
    # Total kills analyzed
    total_kills_analyzed: int = 0

    def get_median(self, steamid: int) -> float | None:
        """Get median CP error for a player."""
        values = self.player_cp_values.get(steamid, [])
        return float(np.median(values)) if values else None

    def get_mean(self, steamid: int) -> float | None:
        """Get mean CP error for a player."""
        values = self.player_cp_values.get(steamid, [])
        return float(np.mean(values)) if values else None


def compute_ttd_vectorized(
    kills_df: pd.DataFrame, damages_df: pd.DataFrame, tick_rate: int = CS2_TICK_RATE
) -> TTDMetrics:
    """
    Compute Time to Damage using vectorized pandas operations.

    This is significantly faster than per-kill loops because:
    1. Uses groupby to find first damage per engagement
    2. Merges DataFrames instead of filtering per kill
    3. Computes TTD in bulk using vectorized subtraction

    Args:
        kills_df: DataFrame with kills (needs: tick, attacker_steamid, victim_steamid)
        damages_df: DataFrame with damages (needs: tick, attacker_steamid, user_steamid/victim_steamid)
        tick_rate: Game tick rate (default 64)

    Returns:
        TTDMetrics with per-player TTD values
    """
    result = TTDMetrics()

    if kills_df.empty or damages_df.empty:
        logger.debug("No kills or damages for TTD computation")
        return result

    ms_per_tick = 1000 / tick_rate

    # Find column names
    def find_col(df: pd.DataFrame, options: list[str]) -> str | None:
        for col in options:
            if col in df.columns:
                return col
        return None

    # Column mappings
    kill_att_col = find_col(kills_df, ["attacker_steamid", "attacker_steam_id"])
    kill_vic_col = find_col(kills_df, ["victim_steamid", "user_steamid", "victim_steam_id"])
    dmg_att_col = find_col(damages_df, ["attacker_steamid", "attacker_steam_id"])
    dmg_vic_col = find_col(damages_df, ["user_steamid", "victim_steamid", "victim_steam_id"])

    if not all([kill_att_col, kill_vic_col, dmg_att_col, dmg_vic_col]):
        logger.warning("Missing required columns for TTD computation")
        return result

    # Standardize column names
    # IMPORTANT: Use int64, NOT float, for steamid columns.
    # Steam IDs are 17-digit integers (e.g. 76561198073476793).
    # float64 only preserves ~15.9 significant digits, so
    # int(float(76561198073476793)) == 76561198073476800 -- wrong!
    # This precision loss causes player lookups to fail silently.
    # Convert steamids WITHOUT float64 intermediate â€” pd.to_numeric goes through
    # float64 which corrupts 17-digit Steam64 IDs (precision loss).
    from opensight.core.parser import steamid_series_to_int

    kills = kills_df[[kill_att_col, kill_vic_col, "tick"]].copy()
    kills.columns = ["attacker_id", "victim_id", "kill_tick"]
    kills["attacker_id"] = steamid_series_to_int(kills["attacker_id"])
    kills["victim_id"] = steamid_series_to_int(kills["victim_id"])

    damages = damages_df[[dmg_att_col, dmg_vic_col, "tick"]].copy()
    damages.columns = ["attacker_id", "victim_id", "dmg_tick"]
    damages["attacker_id"] = steamid_series_to_int(damages["attacker_id"])
    damages["victim_id"] = steamid_series_to_int(damages["victim_id"])

    # For each kill, find the first damage from the same attacker to same victim
    # that occurred within an engagement window BEFORE the kill tick.
    # Without a window, damage from previous rounds contaminates the TTD,
    # producing enormous values (e.g. 60000ms) that get filtered out,
    # causing ALL TTD values to be null.

    # Maximum engagement window: 5 seconds before the kill (matches non-vectorized path)
    max_engagement_ticks = int(5000 / ms_per_tick) + 1

    # Step 1: Create a unique engagement key
    kills["engagement_idx"] = range(len(kills))

    # Step 2: Merge kills with all damage events for the same attacker-victim pair
    merged = kills.merge(damages, on=["attacker_id", "victim_id"], how="left")

    # Step 3: Filter to damages within the engagement window before the kill
    # (not just "before kill" -- must be within max_engagement_ticks)
    engagement_start = merged["kill_tick"] - max_engagement_ticks
    merged = merged[
        (merged["dmg_tick"] <= merged["kill_tick"]) & (merged["dmg_tick"] >= engagement_start)
    ]

    if merged.empty:
        logger.debug("No damage events found within engagement window before kills")
        return result

    # Step 4: For each engagement, get the FIRST damage tick within the window
    first_damage = (
        merged.groupby("engagement_idx")
        .agg(
            attacker_id=("attacker_id", "first"),
            kill_tick=("kill_tick", "first"),
            first_dmg_tick=("dmg_tick", "min"),
        )
        .reset_index()
    )

    # Step 5: Compute TTD in bulk (vectorized)
    first_damage["ttd_ticks"] = first_damage["kill_tick"] - first_damage["first_dmg_tick"]
    first_damage["ttd_ms"] = first_damage["ttd_ticks"] * ms_per_tick

    # Step 6: Filter valid TTD values and identify prefires
    valid_ttd = first_damage[
        (first_damage["ttd_ms"] >= TTD_MIN_MS) & (first_damage["ttd_ms"] <= TTD_MAX_MS)
    ]
    prefires = first_damage[first_damage["ttd_ms"] < TTD_MIN_MS]

    # Step 7: Group by player and collect results
    player_ttd_values: dict[int, list[float]] = {}
    player_prefire_counts: dict[int, int] = {}

    for attacker_id, group in valid_ttd.groupby("attacker_id"):
        sid = int(attacker_id)
        player_ttd_values[sid] = group["ttd_ms"].tolist()

    for attacker_id, group in prefires.groupby("attacker_id"):
        sid = int(attacker_id)
        player_prefire_counts[sid] = len(group)

    result.player_ttd_values = player_ttd_values
    result.player_prefire_counts = player_prefire_counts
    result.total_engagements = len(first_damage)

    logger.info(
        f"TTD computed (vectorized): {result.total_engagements} engagements, "
        f"{sum(len(v) for v in player_ttd_values.values())} valid TTD values"
    )

    return result


def compute_view_vectors_batch(pitch_deg: np.ndarray, yaw_deg: np.ndarray) -> np.ndarray:
    """
    Convert pitch/yaw angles to view direction vectors in batch.

    Uses numpy broadcasting for fast computation on arrays.

    Args:
        pitch_deg: Array of pitch angles in degrees (N,)
        yaw_deg: Array of yaw angles in degrees (N,)

    Returns:
        Array of view direction vectors (N, 3)
    """
    pitch_rad = np.radians(pitch_deg)
    yaw_rad = np.radians(yaw_deg)

    # View vector from Euler angles (CS2 coordinate system)
    view_x = np.cos(yaw_rad) * np.cos(pitch_rad)
    view_y = np.sin(yaw_rad) * np.cos(pitch_rad)
    view_z = -np.sin(pitch_rad)

    return np.column_stack([view_x, view_y, view_z])


def compute_angular_errors_batch(
    attacker_pos: np.ndarray,
    attacker_pitch: np.ndarray,
    attacker_yaw: np.ndarray,
    victim_pos: np.ndarray,
) -> np.ndarray:
    """
    Compute angular errors between view direction and target in batch.

    This is the vectorized version of _calculate_angular_error.

    Args:
        attacker_pos: Array of attacker positions (N, 3)
        attacker_pitch: Array of attacker pitch angles (N,)
        attacker_yaw: Array of attacker yaw angles (N,)
        victim_pos: Array of victim positions (N, 3)

    Returns:
        Array of angular errors in degrees (N,)
    """
    # Compute view direction vectors
    view_vecs = compute_view_vectors_batch(attacker_pitch, attacker_yaw)

    # Compute ideal direction vectors (attacker -> victim)
    direction = victim_pos - attacker_pos
    distances = np.linalg.norm(direction, axis=1, keepdims=True)

    # Avoid division by zero
    distances = np.maximum(distances, 1e-10)
    ideal_vecs = direction / distances

    # Compute dot products (element-wise row dot product)
    dots = np.sum(view_vecs * ideal_vecs, axis=1)

    # Clamp to [-1, 1] for numerical stability
    dots = np.clip(dots, -1.0, 1.0)

    # Compute angles
    angular_errors = np.degrees(np.arccos(dots))

    return angular_errors


def compute_cp_vectorized(kills: list, player_ids: set[int]) -> CPMetrics:
    """
    Compute Crosshair Placement using vectorized numpy operations (Leetify-style).

    IMPROVED IMPLEMENTATION:
    - Filters out knife/grenade kills (only guns count for CP)
    - Uses vectorized numpy for fast batch computation

    This is significantly faster than per-kill loops because:
    1. Extracts all position data into arrays upfront
    2. Uses numpy broadcasting for angle calculations
    3. Computes all angular errors in one batch operation

    Args:
        kills: List of KillEvent objects with position data
        player_ids: Set of player steam IDs to compute for

    Returns:
        CPMetrics with per-player CP values
    """
    result = CPMetrics()

    # Filter kills with position data AND valid weapons (no knives/grenades)
    filtered_weapon = 0
    kills_with_pos = []
    for k in kills:
        # Check position data
        if k.attacker_x is None or k.attacker_pitch is None or k.victim_x is None:
            continue

        # WEAPON FILTER: Only primary/secondary weapons count for CP
        weapon = getattr(k, "weapon", None)
        if not _is_valid_cp_weapon(weapon):
            filtered_weapon += 1
            continue

        kills_with_pos.append(k)

    if filtered_weapon > 0:
        logger.debug(f"CP: Filtered {filtered_weapon} kills with invalid weapons (knife/grenade)")

    if not kills_with_pos:
        logger.debug("No kills with position data for CP computation")
        return result

    # Extract data into arrays
    n_kills = len(kills_with_pos)

    attacker_ids = np.array([k.attacker_steamid for k in kills_with_pos])

    # Position arrays (+64 for eye height)
    attacker_pos = np.column_stack(
        [
            [k.attacker_x for k in kills_with_pos],
            [k.attacker_y for k in kills_with_pos],
            [k.attacker_z + 64 if k.attacker_z else 64 for k in kills_with_pos],
        ]
    )

    victim_pos = np.column_stack(
        [
            [k.victim_x for k in kills_with_pos],
            [k.victim_y for k in kills_with_pos],
            [k.victim_z + 64 if k.victim_z else 64 for k in kills_with_pos],
        ]
    )

    # Angle arrays
    attacker_pitch = np.array([k.attacker_pitch or 0.0 for k in kills_with_pos])
    attacker_yaw = np.array([k.attacker_yaw or 0.0 for k in kills_with_pos])

    # Filter out invalid positions (zeros) and distance check
    distances = np.linalg.norm(victim_pos - attacker_pos, axis=1)
    valid_mask = (
        ((np.abs(attacker_pos[:, 0]) > 0.1) | (np.abs(attacker_pos[:, 1]) > 0.1))
        & ((np.abs(victim_pos[:, 0]) > 0.1) | (np.abs(victim_pos[:, 1]) > 0.1))
        & (distances >= CP_MIN_DISTANCE)
        & (distances <= CP_MAX_DISTANCE)
    )

    if not np.any(valid_mask):
        logger.debug("No valid positions for CP computation")
        return result

    # Apply mask
    attacker_ids_valid = attacker_ids[valid_mask]
    attacker_pos_valid = attacker_pos[valid_mask]
    victim_pos_valid = victim_pos[valid_mask]
    attacker_pitch_valid = attacker_pitch[valid_mask]
    attacker_yaw_valid = attacker_yaw[valid_mask]

    # Compute angular errors in batch
    angular_errors = compute_angular_errors_batch(
        attacker_pos_valid, attacker_pitch_valid, attacker_yaw_valid, victim_pos_valid
    )

    # Group by player
    player_cp_values: dict[int, list[float]] = {}

    for i, sid in enumerate(attacker_ids_valid):
        sid = int(sid)
        if sid in player_ids:
            if sid not in player_cp_values:
                player_cp_values[sid] = []
            # Filter out invalid errors (NaN or very large)
            error = angular_errors[i]
            if np.isfinite(error) and error < 180:
                player_cp_values[sid].append(float(error))

    result.player_cp_values = player_cp_values
    result.total_kills_analyzed = len(attacker_ids_valid)

    logger.info(
        f"CP computed (vectorized): {n_kills} kills after weapon filter, "
        f"{result.total_kills_analyzed} with valid positions, "
        f"{sum(len(v) for v in player_cp_values.values())} valid CP values"
    )

    return result


def compute_cp_from_dataframe_vectorized(kills_df: pd.DataFrame, player_ids: set[int]) -> CPMetrics:
    """
    Compute Crosshair Placement from DataFrame using vectorized operations (Leetify-style).

    IMPROVED IMPLEMENTATION:
    - Filters out knife/grenade kills (only guns count for CP)
    - Distance filtering for valid engagements

    Alternative to compute_cp_vectorized when working with DataFrames.

    Args:
        kills_df: DataFrame with kills (needs position and angle columns)
        player_ids: Set of player steam IDs to compute for

    Returns:
        CPMetrics with per-player CP values
    """
    result = CPMetrics()

    if kills_df.empty:
        return result

    # Find column names
    def find_col(df: pd.DataFrame, options: list[str]) -> str | None:
        for col in options:
            if col in df.columns:
                return col
        return None

    att_id_col = find_col(kills_df, ["attacker_steamid", "attacker_steam_id"])
    att_x = find_col(kills_df, ["attacker_X", "attacker_x"])
    att_y = find_col(kills_df, ["attacker_Y", "attacker_y"])
    att_z = find_col(kills_df, ["attacker_Z", "attacker_z"])
    att_pitch = find_col(kills_df, ["attacker_pitch"])
    att_yaw = find_col(kills_df, ["attacker_yaw"])
    vic_x = find_col(kills_df, ["victim_X", "victim_x", "user_X", "user_x"])
    vic_y = find_col(kills_df, ["victim_Y", "victim_y", "user_Y", "user_y"])
    vic_z = find_col(kills_df, ["victim_Z", "victim_z", "user_Z", "user_z"])
    weapon_col = find_col(kills_df, ["weapon", "weapon_name"])

    required_cols = [att_id_col, att_x, att_y, att_z, att_pitch, att_yaw, vic_x, vic_y, vic_z]
    if not all(required_cols):
        logger.warning("Missing required columns for vectorized CP computation")
        return result

    # Create working copy with standardized columns
    cols_to_copy = [att_id_col, att_x, att_y, att_z, att_pitch, att_yaw, vic_x, vic_y, vic_z]
    if weapon_col:
        cols_to_copy.append(weapon_col)

    df = kills_df[cols_to_copy].copy()

    if weapon_col:
        df.columns = [
            "attacker_id",
            "att_x",
            "att_y",
            "att_z",
            "att_pitch",
            "att_yaw",
            "vic_x",
            "vic_y",
            "vic_z",
            "weapon",
        ]
    else:
        df.columns = [
            "attacker_id",
            "att_x",
            "att_y",
            "att_z",
            "att_pitch",
            "att_yaw",
            "vic_x",
            "vic_y",
            "vic_z",
        ]

    original_count = len(df)

    # Filter valid rows (non-null positions)
    df = df.dropna(
        subset=["att_x", "att_y", "att_z", "att_pitch", "att_yaw", "vic_x", "vic_y", "vic_z"]
    )

    # WEAPON FILTER: Only primary/secondary weapons count for CP
    if "weapon" in df.columns:
        weapon_mask = df["weapon"].apply(lambda w: _is_valid_cp_weapon(w))
        filtered_weapon = (~weapon_mask).sum()
        df = df[weapon_mask]
        if filtered_weapon > 0:
            logger.debug(
                f"CP: Filtered {filtered_weapon} kills with invalid weapons (knife/grenade)"
            )

    # Filter out zero positions
    valid_mask = ((df["att_x"].abs() > 0.1) | (df["att_y"].abs() > 0.1)) & (
        (df["vic_x"].abs() > 0.1) | (df["vic_y"].abs() > 0.1)
    )
    df = df[valid_mask]

    if df.empty:
        logger.debug("No valid positions for vectorized CP computation")
        return result

    # Extract arrays
    attacker_ids = df["attacker_id"].values
    attacker_pos = np.column_stack(
        [
            df["att_x"].values,
            df["att_y"].values,
            df["att_z"].values + 64,  # Eye height
        ]
    )
    victim_pos = np.column_stack([df["vic_x"].values, df["vic_y"].values, df["vic_z"].values + 64])
    attacker_pitch = df["att_pitch"].values
    attacker_yaw = df["att_yaw"].values

    # Distance filter
    distances = np.linalg.norm(victim_pos - attacker_pos, axis=1)
    distance_mask = (distances >= CP_MIN_DISTANCE) & (distances <= CP_MAX_DISTANCE)

    attacker_ids = attacker_ids[distance_mask]
    attacker_pos = attacker_pos[distance_mask]
    victim_pos = victim_pos[distance_mask]
    attacker_pitch = attacker_pitch[distance_mask]
    attacker_yaw = attacker_yaw[distance_mask]

    if len(attacker_ids) == 0:
        logger.debug("No kills within valid distance range for CP computation")
        return result

    # Compute angular errors in batch
    angular_errors = compute_angular_errors_batch(
        attacker_pos, attacker_pitch, attacker_yaw, victim_pos
    )

    # Group by player
    player_cp_values: dict[int, list[float]] = {}

    for i, sid in enumerate(attacker_ids):
        sid = int(sid)  # Keep as int (float conversion loses precision on 17-digit Steam IDs)
        if sid in player_ids:
            if sid not in player_cp_values:
                player_cp_values[sid] = []
            error = angular_errors[i]
            if np.isfinite(error) and error < 180:
                player_cp_values[sid].append(float(error))

    result.player_cp_values = player_cp_values
    result.total_kills_analyzed = len(attacker_ids)

    logger.info(
        f"CP computed (DataFrame vectorized): {original_count} original kills, "
        f"{result.total_kills_analyzed} after filtering, "
        f"{sum(len(v) for v in player_cp_values.values())} valid CP values"
    )

    return result


# ============================================================================
# Metrics Caching
# ============================================================================


@dataclass
class MetricsCache:
    """Cache for computed metrics."""

    demo_hash: str
    demo_path: str
    ttd_metrics: TTDMetrics | None = None
    cp_metrics: CPMetrics | None = None
    computed_metrics: MetricType = MetricType.NONE


class MetricsCacheManager:
    """
    Manages caching of computed metrics to avoid recomputation.

    Caches are keyed by demo file hash (size + first/last bytes).
    """

    def __init__(self, cache_dir: Path | None = None):
        """
        Initialize cache manager.

        Args:
            cache_dir: Directory to store cache files. If None, uses in-memory only.
        """
        self.cache_dir = cache_dir
        self._memory_cache: dict[str, MetricsCache] = {}

    def _compute_demo_hash(self, demo_path: Path) -> str:
        """Compute a quick hash for the demo file."""
        try:
            if not demo_path.exists():
                # Fallback for test scenarios
                return hashlib.md5(str(demo_path).encode(), usedforsecurity=False).hexdigest()[:16]

            stat = demo_path.stat()
            hasher = hashlib.md5(usedforsecurity=False)
            hasher.update(str(stat.st_size).encode())
            hasher.update(str(stat.st_mtime).encode())

            # Read first and last 1KB
            file_size = stat.st_size
            with open(demo_path, "rb") as f:
                hasher.update(f.read(1024))
                if file_size > 2048:
                    f.seek(-1024, 2)  # Seek 1KB from end
                    hasher.update(f.read(1024))

            return hasher.hexdigest()[:16]
        except OSError as e:
            logger.debug(f"Could not compute demo hash: {e}")
            return hashlib.md5(str(demo_path).encode(), usedforsecurity=False).hexdigest()[:16]

    def get_cache(self, demo_path: Path) -> MetricsCache | None:
        """Get cached metrics for a demo file."""
        demo_hash = self._compute_demo_hash(demo_path)

        # Check memory cache
        if demo_hash in self._memory_cache:
            logger.debug(f"Cache hit (memory): {demo_hash}")
            return self._memory_cache[demo_hash]

        # Check disk cache
        if self.cache_dir:
            cache_file = self.cache_dir / f"{demo_hash}.metrics.json"
            if cache_file.exists():
                try:
                    with open(cache_file) as f:
                        data = json.load(f)
                    cache = self._deserialize_cache(data)
                    self._memory_cache[demo_hash] = cache
                    logger.debug(f"Cache hit (disk): {demo_hash}")
                    return cache
                except Exception as e:
                    logger.warning(f"Failed to load cache: {e}")

        return None

    def save_cache(self, demo_path: Path, cache: MetricsCache) -> None:
        """Save computed metrics to cache."""
        demo_hash = self._compute_demo_hash(demo_path)
        cache.demo_hash = demo_hash
        cache.demo_path = str(demo_path)

        # Save to memory
        self._memory_cache[demo_hash] = cache

        # Save to disk
        if self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            cache_file = self.cache_dir / f"{demo_hash}.metrics.json"
            try:
                with open(cache_file, "w") as f:
                    json.dump(self._serialize_cache(cache), f)
                logger.debug(f"Cache saved: {demo_hash}")
            except Exception as e:
                logger.warning(f"Failed to save cache: {e}")

    def _serialize_cache(self, cache: MetricsCache) -> dict:
        """Serialize cache to JSON-compatible dict."""
        data = {
            "demo_hash": cache.demo_hash,
            "demo_path": cache.demo_path,
            "computed_metrics": cache.computed_metrics.value,
        }

        if cache.ttd_metrics:
            data["ttd_metrics"] = {
                "player_ttd_values": {
                    str(k): v for k, v in cache.ttd_metrics.player_ttd_values.items()
                },
                "player_prefire_counts": {
                    str(k): v for k, v in cache.ttd_metrics.player_prefire_counts.items()
                },
                "total_engagements": cache.ttd_metrics.total_engagements,
            }

        if cache.cp_metrics:
            data["cp_metrics"] = {
                "player_cp_values": {
                    str(k): v for k, v in cache.cp_metrics.player_cp_values.items()
                },
                "total_kills_analyzed": cache.cp_metrics.total_kills_analyzed,
            }

        return data

    def _deserialize_cache(self, data: dict) -> MetricsCache:
        """Deserialize cache from JSON-compatible dict."""
        cache = MetricsCache(
            demo_hash=data["demo_hash"],
            demo_path=data["demo_path"],
            computed_metrics=MetricType(data["computed_metrics"]),
        )

        if "ttd_metrics" in data:
            ttd_data = data["ttd_metrics"]
            cache.ttd_metrics = TTDMetrics(
                player_ttd_values={int(k): v for k, v in ttd_data["player_ttd_values"].items()},
                player_prefire_counts={
                    int(k): v for k, v in ttd_data["player_prefire_counts"].items()
                },
                total_engagements=ttd_data["total_engagements"],
            )

        if "cp_metrics" in data:
            cp_data = data["cp_metrics"]
            cache.cp_metrics = CPMetrics(
                player_cp_values={int(k): v for k, v in cp_data["player_cp_values"].items()},
                total_kills_analyzed=cp_data["total_kills_analyzed"],
            )

        return cache


# Global cache manager instance
_cache_manager: MetricsCacheManager | None = None


def get_cache_manager() -> MetricsCacheManager:
    """Get or create the global cache manager."""
    global _cache_manager
    if _cache_manager is None:
        _cache_manager = MetricsCacheManager()
    return _cache_manager


def set_cache_directory(cache_dir: Path) -> None:
    """Set the cache directory for metrics caching."""
    global _cache_manager
    _cache_manager = MetricsCacheManager(cache_dir)


# ============================================================================
# Configurable Metrics Computation
# ============================================================================


class OptimizedMetricsComputer:
    """
    Optimized metrics computation with configurable metric selection.

    Usage:
        computer = OptimizedMetricsComputer(demo_data)
        computer.compute(MetricType.TTD | MetricType.CP)

        # Get results
        ttd = computer.get_ttd_for_player(steamid)
        cp = computer.get_cp_for_player(steamid)
    """

    def __init__(self, demo_data: Any, use_cache: bool = True):
        """
        Initialize computer.

        Args:
            demo_data: DemoData from parser
            use_cache: Whether to use caching
        """
        self.data = demo_data
        self.use_cache = use_cache
        self._ttd_metrics: TTDMetrics | None = None
        self._cp_metrics: CPMetrics | None = None
        self._computed_metrics = MetricType.NONE

    def compute(self, metrics: MetricType = MetricType.FULL) -> None:
        """
        Compute requested metrics.

        Args:
            metrics: MetricType flags for which metrics to compute
        """
        # Check cache
        if self.use_cache:
            cache = get_cache_manager().get_cache(self.data.file_path)
            if cache:
                if MetricType.TTD in metrics and cache.ttd_metrics:
                    self._ttd_metrics = cache.ttd_metrics
                    self._computed_metrics |= MetricType.TTD
                if MetricType.CP in metrics and cache.cp_metrics:
                    self._cp_metrics = cache.cp_metrics
                    self._computed_metrics |= MetricType.CP

        # Compute missing metrics
        if MetricType.TTD in metrics and not (self._computed_metrics & MetricType.TTD):
            self._compute_ttd()

        if MetricType.CP in metrics and not (self._computed_metrics & MetricType.CP):
            self._compute_cp()

        # Save to cache
        if self.use_cache:
            cache = MetricsCache(
                demo_hash="",
                demo_path=str(self.data.file_path),
                ttd_metrics=self._ttd_metrics,
                cp_metrics=self._cp_metrics,
                computed_metrics=self._computed_metrics,
            )
            get_cache_manager().save_cache(self.data.file_path, cache)

    def _compute_ttd(self) -> None:
        """Compute TTD metrics using vectorized implementation."""
        self._ttd_metrics = compute_ttd_vectorized(
            self.data.kills_df, self.data.damages_df, self.data.tick_rate
        )
        self._computed_metrics |= MetricType.TTD

    def _compute_cp(self) -> None:
        """Compute CP metrics using vectorized implementation."""
        player_ids = set(self.data.player_names.keys())

        # Try KillEvent objects first (preferred)
        if self.data.kills:
            self._cp_metrics = compute_cp_vectorized(self.data.kills, player_ids)
        # Fallback to DataFrame
        elif not self.data.kills_df.empty:
            self._cp_metrics = compute_cp_from_dataframe_vectorized(self.data.kills_df, player_ids)
        else:
            self._cp_metrics = CPMetrics()

        self._computed_metrics |= MetricType.CP

    def get_ttd_values(self, steamid: int) -> list[float]:
        """Get TTD values for a player."""
        if not self._ttd_metrics:
            return []
        return self._ttd_metrics.player_ttd_values.get(steamid, [])

    def get_ttd_median(self, steamid: int) -> float | None:
        """Get median TTD for a player."""
        if not self._ttd_metrics:
            return None
        return self._ttd_metrics.get_median(steamid)

    def get_prefire_count(self, steamid: int) -> int:
        """Get prefire count for a player."""
        if not self._ttd_metrics:
            return 0
        return self._ttd_metrics.player_prefire_counts.get(steamid, 0)

    def get_cp_values(self, steamid: int) -> list[float]:
        """Get CP values for a player."""
        if not self._cp_metrics:
            return []
        return self._cp_metrics.player_cp_values.get(steamid, [])

    def get_cp_median(self, steamid: int) -> float | None:
        """Get median CP error for a player."""
        if not self._cp_metrics:
            return None
        return self._cp_metrics.get_median(steamid)

    @property
    def ttd_metrics(self) -> TTDMetrics | None:
        """Get TTD metrics."""
        return self._ttd_metrics

    @property
    def cp_metrics(self) -> CPMetrics | None:
        """Get CP metrics."""
        return self._cp_metrics
